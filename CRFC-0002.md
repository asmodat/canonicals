# CRFC-0002 - Data de-duplication & chunking for scalable DHT systems

# Overview

Optimizing storage space should be one of the most important features of any system archiving large amounts of data that must remain persistent permanently or for long periods of time. One of the most common ways to achieve deduplication is chunking large files into multiple parts, indexing all the parts and only storing parts which are different every time new file is uploaded. One example of such system is AWS S3 which forces large files ([above 5GB](https://docs.aws.amazon.com/AmazonS3/latest/userguide/upload-objects.html#:~:text=With%20a%20single%20PUT%20operation%2C%20you%20can%20upload%20a%20single%20object%20up%20to%205%20GB%20in%20size.)) to be chunked before upload. Of course de-duplication is not the only argument for chunking, another one is preventing situations in which networking issues force the client or intermediary gateways or nodes to repeat data transmission due to failed connection or aggressive firewall rules which can interrupt the upload/download.

In this CRFC we will will explore various data de-duplication methods, compression and more.

**Considerations**

- The [largest commercially available hard drive](https://en.wikipedia.org/wiki/History_of_hard_disk_drives#:~:text=As%20of%20August%202022%2C%20the,and%205TB%20as%20external%20drives.) can store up to 22 TB
- The [fastest commercially available internet connection](https://www.business.att.com/products/att-dedicated-internet.html?WT.srch=1&source=EBpBNV00000PSM00P&wtExtndSource=PS_21_SMB_FIBER&wtpdsrchprg=AT%2526T%2520ABS&wtpdsrchgp=ABS_SEARCH&wtPaidSearchTerm=business%20internet&wtpdsrchpcmt=business%20internet&kid=kwd-21206153&cid=17665517659&TFN=B2B&ds_eid=700000001878296&ds_cid=71700000097510884&ds_agid=58700007917328010&ds_kids=p71981000459&gclid=CjwKCAjwzNOaBhAcEiwAD7Tb6CkiMyd6r-ZUyTNgUaQ_l_yzhA6uO9SUaOnEU5z4rQH0q7gBXsiSIBoCFjYQAvD_BwE&gclsrc=aw.ds) can achieve a speed of up to 125 GB/s
- The [fastest commercially available NVME drive](https://www.gamingpcbuilder.com/best-m-2-nvme-ssd/#:~:text=max.%2C%20MB/s)-,6%2C500,-3%2C300) can achieve write speed of up to 6 GB/s
- The [BLAKE3 hashing algorithm can achieve speed of 6.8 GB/s](https://www.linuxadictos.com/en/blake3-a-fast-and-parallelizable-secure-cryptographic-hash-function.html) on 2nd gen Intel Xeon 3.9 GHz per each 1 thread
- [Fastest lossless universal compression algorithm](https://engineering.fb.com/2016/08/31/core-data/smaller-and-faster-data-compression-with-zstandard/#:~:text=The%20fastest%20algorithm%2C%20lz4%2C%20results,from%20a%20slow%20compression%20speed.) can achieve [decompression speed of 1.7 GB/s](https://github.com/facebook/zstd) on Core i7-9700K CPU @ 4.9GHz
- [Maximum theoretical limit of TCP connections on linux](https://stackoverflow.com/questions/2332741/what-is-the-theoretical-maximum-number-of-open-tcp-connections-that-a-modern-lin#:~:text=So%20the%20real%20limit%20is%20file%20descriptors) is 64k for a single port and up to 300k on multiple ports.
- [Maximum number of parallel TCP connections on windows is 3975 while practical around 1000](https://droidrant.com/how-many-tcp-connections-can-a-windows-server-handle/#:~:text=the%20theoretical%20maximum%20Windows%20server%20can%20handle%20is%2025%2C000%20TCP%20connections%2C%20while%20the%20practical%20limit%20is%20approximately%20one%20thousand.)
- [Maximum number of parallel TCP connections in chrome browser is 6](https://bugs.chromium.org/p/chromium/issues/detail?id=12066#:~:text=Chrome%20limits%20the%20number%20of%20connections%20per%20%22group%22%20to%206.)
- By [default linux limits number of concurrent TCP connections to the same IP & port](https://stackoverflow.com/questions/410616/increasing-the-maximum-number-of-tcp-ip-connections-in-linux#:~:text=(61000%20%2D%2032768)%20/%2060%20%3D-,470,-sockets%20per%20second.%20If) to 470/s
- [Raw 8k video footage at 75 FPS consumes 2GB/s](https://www.signiant.com/resources/tech-articles/file-size-growth-bandwidth-conundrum/#:~:text=7.29%20TB.%20That%E2%80%99s-,121.5%20GB%20per%20minute,-for%20raw%208K)
- [Recommended YouTube bitrate is 30 MB/s for 8k 60 FPS video upload](https://support.google.com/youtube/answer/1722171?hl=en#zippy=%2Cbitrate:~:text=120%20to-,240%20Mbps,-2160p%20(4K))
- [Size of H.264/AVC codec 8k 60 FPS video file is 68 MB/s](https://www.macxdvd.com/mac-video-converter-pro/compress-reduce-8k-video-size.htm#:~:text=8K%20video%20size%20with%201%20minute%20length%3A%20688MB)
- [Delta Compression](https://en.wikipedia.org/wiki/Delta_encoding)

# Chunk Sizes

### Hashing Algorithm

Size of chunks should strictly depend on the size of the hashes which will be uniquely identifying files, the longer the hash the larger the chunks should be so that reference indexes identified by the hashes represent a relatively small portion of those files. For the purpose of this experiment let’s choose [BLAKE3 hash which consumes 32 Bytes, offers parallelization and on average is over 10x faster to execute then SHA-256](https://www.linuxadictos.com/en/blake3-a-fast-and-parallelizable-secure-cryptographic-hash-function.html). 

The smallest multipart chunk of an object that AWS S3 allows to upload is 5MB, this implies that the BLAKE3 reference hash of such object would only result in extra 0.0006% of data being stored. As long as an average probability of the de-duplication in the system is greater than overhead it adds, it will be possible to save storage space in the system. It should be noted that the larger the chunk the probability of the de-duplication will drastically fall. Only long running statistics on the real life sample data could provide us with good estimations in regards to what is the ideal chunk (default) size as that will strictly depend on the users.

### Maximum File & Minimum Chunk Size

Considering that largest hard drive as of [august 2022 can store up to 22TB](https://en.wikipedia.org/wiki/History_of_hard_disk_drives#:~:text=As%20of%20August%202022%2C%20the,and%205TB%20as%20external%20drives.) and that [storage technologies are not achieving ideal Moore's Law metrics](https://aip.scitation.org/doi/10.1063/1.5007621) (33% increase per year) we will unlikely see larger than 1PB consumer drives before the year 2036, 1 EB by 2060, 1 ZB by 2085 and 1 YB by 2109. It should be reasonable to assume that planning for storage of files larger than 1PB is unnecessary for any consumer applications however this doesn’t mean that the protocol should not be sufficiently scalable to facilitate maximum possible file sizes rather than being limited to AWS S3 like constraints (5 TB max upload) as that is a limiting factor for many potential commercial applications (e.g. storing, accessing & processing high volume category of “[big data](https://en.wikipedia.org/wiki/Big_data)”).

Assuming default chunk size of 5MB, each metadata chunk could store `5MB / 32 B = 163'840` indexes, which can represent up to `163'840 * 5MB = 800 GB` of data. The main limiting factor is however the maximum number of TCP connections that could in parallel be requesting those chunks of data. That is 300k for linux, 1000 in case of operating systems such as Windows and only 6 in case of browser applications such as Chrome. Assuming `100ms` latency for requesting a chunk from a random P2P node, maximum download speed of `5MB * 6 * 1000ms/100ms = 300MB/s` would be achieved. Since a raw 75 FPS 8k video stream will consume up to 2GB/s we will require a minimum of `2GB / (6 * 1000ms/100ms) = 34.1(3) MB` chunk sizes to facilitate that transfer within the chrome browser application, to reach full 120 FPS we would thus need a chunk of no less than 54.6 MB.

The above thought experiment implies that nodes in the network should either facilitate file re-assembly, decompression (or partial decompression) and streaming while acting as gateways or that minimum chunk size should be no less than 55 MB to facilitate practical use cases such as video streaming. Taking into account that a p2p node running on the linux operating system should in practical scenario be able to access up to `65535 * 5MB * 1000ms/100ms = 3.1 TB/s` of data (in the case of 5MB chunks) to serve up to ~999 clients streaming 8k video 120 FPS (not taking into account networking, storage and computational costs and capabilities), then **the gateway type implementation is much more logical and implies that chunk sizes should be fixed to a specific length** (smallest possible) rather then variable like in the case of AWS.

To discover the location of a metadata for any particular file in the DHT, we can calculate a BLAKE3 hash of the list of all the hashes of all chunks which constitute that particular file. Since the user might only know the file hash and not know the hashes of all the chunks which constitute that file, we can “store” the list of chunks as a chunk itself to constitute its presence in the DHT. This method will however limit the maximum file size that can be stored in the protocol since only one chunk could store indexes to all other parts of the file. If the fixed chunk size is set to 5MB, then maximum file size in the protocol would be limited to 800 GB. To match capabilities of AWS (5 TB max upload) we will need to utilize a metadata in order to inform the user how to interpret content of the files, for example a 8TB file can be represented as recursive list of hashes to 10 individual 800 GB files. Given such approach of recursive file references there can be no upper limit in regards to maximum file size that can be stored.

### Multi-key Lookup

One of the largest issues of chunking is the growing number of lookups required with the growing number of chunks. One possible solution to this problem is a modification of how DHT queries behave, that is instead of requesting a single index one at the time, we can request from the node to point us towards a list of IP addresses in relation to all chunks we are interested in simultaneously or just the specific number of chunks depending on the client networking speed and lookup time. We might also request a series of chunks to be looked up in batches of specific order rather than randomly, allowing us to create an illusion of seamless download through the on-the-go recombination of the chunks. 

Another argument for multi-key lookups and chunking is the increased speed of large file discovery and download initiation time since the probability of randomly finding a random chunk on the first trial increases with total number of chunks we are searching for.

### **Highlights**

- Hashing algorithm
    - BLAKE3 is more than 10x faster then SHA-256 (used by IPFS) while remaining cryptographically secure and offering similar collision resistance
- Maximum file size
    - Chunk size should be fixed to 5MB
- Chunk number and DHT lookup time
    - Ordered-multi-key lookup during each file search, batched depending on the networking speed

# Metadata

The core protocol metadata should be used for interpreting “chunked” data, since methods of compression, or de-duplication might change over time and the metadata should provide a future-proof mechanism for ensuring that the chunks can be decoded.

Each individual node storing data should be individually responsible for persisting chunks based on the metadata in such a manner to maximally optimize decoding, networking capacity and/or storage space. A dedicated process can be run in the background to optimize storage based on the node preferences which can be part of the public node info API response.

Each index in the DHT should correspond to a particular metadata file stored alongside each data chunk as separate file. Since the storage system is so simple no database is needed and both chunks as well as their metadata can be stored directly on the hard drive (e.g. `<hash>.bin` & `<hash>.cmet`). 

**Example structure of the protocol core metadata might look as follows:**

- `version_id` - 2 bytes, defining how to interpret metadata
- `type` - 2 bytes, type of metadata, e.g.
    - `00000000 00000000` -  reserved
    - `00000000 00000001` -  metadata of the file to which chunk indexes can be found in the following <chunk_hash>
    - `00000000 00000010` -  metadata of the file to which reference of the chunk indexes can be found in the following <ref_hash>
    - `00000000 00000011` -  simple chunk binary file
    - …
- `chunk_hash` (optional) - 32 bytes, a key in the DHT which points to the file containing list of chunk hashes constituting a file
- `ref_hash` (optional) - 32 bytes, a key in the DHT which points to the file containing list of file hashes constituting a file
- `users_count` - 2 bytes - number of users interested in persisting a file
    - if no space is left on the node and no users are interested in preserving a file then file can be deleted
- `references_count` - 2 bytes - number of other files/chunks referencing a file
    - if no space is left on the node and no other files/chunks are referencing this chunk then it can be deleted
- `priority` - 2 bytes - defining how node should prioritize access to the file (e.g should file be stored on NVME drive for rapid access and often use or on magnetic disk / tape because it’s rarely used)

# Compression & De-duplication

While de-duplication in the case of using chunks is simple (do not store chunks that already exist in the system) the issue of compression is much more complex. The most versatile compression algo to date is [Zstandard](https://engineering.fb.com/2016/08/31/core-data/smaller-and-faster-data-compression-with-zstandard/#:~:text=The%20fastest%20algorithm%2C%20lz4%2C%20results,from%20a%20slow%20compression%20speed.) developed by Meta (which can be a default for low priority files), however much faster and space efficient algorithms can be created if we can take into account that many chunks might differ by very small number of bytes (single byte or shift of bytes). A dedicated process can be created discovering chunks (on local) and remote nodes than using a [Delta Compression](https://en.wikipedia.org/wiki/Delta_encoding). Since metadata can reference one chunk from another a very storage efficient system can be created.

# Notes

- [Consider a default number of connected peers to be in range of 30-55](https://wiki.theory.org/BitTorrentSpecification#Tracker_Response:~:text=Even%2030%20peers%20is%20plenty%2C%20the%20official%20client%20version%203%20in%20fact%20only%20actively%20forms%20new%20connections%20if%20it%20has%20less%20than%2030%20peers%20and%20will%20refuse%20connections%20if%20it%20has%2055)